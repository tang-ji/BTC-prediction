{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tangji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "print(tf.__version__)\n",
    "\n",
    "import autoencoder\n",
    "import model\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.alternative.me/fng/?limit=1500')\n",
    "data = json.loads(r.text)\n",
    "\n",
    "df_fearGreed = pd.DataFrame(data[\"data\"], columns=[\"timestamp\", \"value\"]).rename(columns={\"timestamp\": \"DATE\", \"value\": \"Fear&Greed\"})\n",
    "df_fearGreed[\"DATE\"] = pd.to_datetime(df_fearGreed[\"DATE\"], unit=\"s\")\n",
    "df_fearGreed[\"Fear&Greed\"] = df_fearGreed[\"Fear&Greed\"].astype(float)\n",
    "df_fearGreed = df_fearGreed.set_index(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2018, 2, 1)\n",
    "end = datetime(2020, 10, 31)\n",
    "Indicators = DataReader([\"CBBTCUSD\", \"CBBCHUSD\", \"CBCCIND\", \"CBETHUSD\", \"sp500\", \"VIXCLS\", \"T10YIE\", \"DGS5\", \"GOLDPMGBD230NLBM\", \"SLVPRUSD\"], 'fred', start, end)\\\n",
    "    .fillna(method=\"pad\").fillna(method=\"backfill\").join(df_fearGreed).fillna(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicators.to_csv('dataset/Indicators.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicators = pd.read_csv('dataset/Indicators.csv', index_col=\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBBTCUSD</th>\n",
       "      <th>CBBCHUSD</th>\n",
       "      <th>CBCCIND</th>\n",
       "      <th>CBETHUSD</th>\n",
       "      <th>sp500</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>T10YIE</th>\n",
       "      <th>DGS5</th>\n",
       "      <th>GOLDPMGBD230NLBM</th>\n",
       "      <th>SLVPRUSD</th>\n",
       "      <th>Fear&amp;Greed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>9014.23</td>\n",
       "      <td>1254.86</td>\n",
       "      <td>4512.850019</td>\n",
       "      <td>1017.48</td>\n",
       "      <td>2821.98</td>\n",
       "      <td>13.47</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1078.58</td>\n",
       "      <td>17.190</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>8787.52</td>\n",
       "      <td>1181.00</td>\n",
       "      <td>4455.297470</td>\n",
       "      <td>911.99</td>\n",
       "      <td>2762.13</td>\n",
       "      <td>17.31</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1071.18</td>\n",
       "      <td>17.135</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>9240.00</td>\n",
       "      <td>1269.71</td>\n",
       "      <td>4655.899898</td>\n",
       "      <td>969.40</td>\n",
       "      <td>2762.13</td>\n",
       "      <td>17.31</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1071.18</td>\n",
       "      <td>17.135</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>8167.91</td>\n",
       "      <td>1156.16</td>\n",
       "      <td>4168.798254</td>\n",
       "      <td>826.00</td>\n",
       "      <td>2762.13</td>\n",
       "      <td>17.31</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1071.18</td>\n",
       "      <td>17.135</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>6905.19</td>\n",
       "      <td>880.00</td>\n",
       "      <td>3520.919532</td>\n",
       "      <td>693.54</td>\n",
       "      <td>2648.94</td>\n",
       "      <td>37.32</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1075.01</td>\n",
       "      <td>16.875</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-27</th>\n",
       "      <td>13698.18</td>\n",
       "      <td>264.53</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>404.55</td>\n",
       "      <td>3390.68</td>\n",
       "      <td>33.35</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1611.65</td>\n",
       "      <td>24.435</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-28</th>\n",
       "      <td>13280.00</td>\n",
       "      <td>268.23</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>388.83</td>\n",
       "      <td>3271.03</td>\n",
       "      <td>40.28</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1593.38</td>\n",
       "      <td>23.925</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-29</th>\n",
       "      <td>13460.00</td>\n",
       "      <td>267.43</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>387.63</td>\n",
       "      <td>3310.11</td>\n",
       "      <td>37.59</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1601.16</td>\n",
       "      <td>23.015</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-30</th>\n",
       "      <td>13582.97</td>\n",
       "      <td>262.16</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>382.66</td>\n",
       "      <td>3269.96</td>\n",
       "      <td>37.59</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1611.24</td>\n",
       "      <td>23.625</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-31</th>\n",
       "      <td>13745.02</td>\n",
       "      <td>262.07</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>385.74</td>\n",
       "      <td>3269.96</td>\n",
       "      <td>37.59</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1611.24</td>\n",
       "      <td>23.625</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CBBTCUSD  CBBCHUSD      CBCCIND  CBETHUSD    sp500  VIXCLS  \\\n",
       "DATE                                                                     \n",
       "2018-02-01   9014.23   1254.86  4512.850019   1017.48  2821.98   13.47   \n",
       "2018-02-02   8787.52   1181.00  4455.297470    911.99  2762.13   17.31   \n",
       "2018-02-03   9240.00   1269.71  4655.899898    969.40  2762.13   17.31   \n",
       "2018-02-04   8167.91   1156.16  4168.798254    826.00  2762.13   17.31   \n",
       "2018-02-05   6905.19    880.00  3520.919532    693.54  2648.94   37.32   \n",
       "...              ...       ...          ...       ...      ...     ...   \n",
       "2020-10-27  13698.18    264.53  3180.272971    404.55  3390.68   33.35   \n",
       "2020-10-28  13280.00    268.23  3180.272971    388.83  3271.03   40.28   \n",
       "2020-10-29  13460.00    267.43  3180.272971    387.63  3310.11   37.59   \n",
       "2020-10-30  13582.97    262.16  3180.272971    382.66  3269.96   37.59   \n",
       "2020-10-31  13745.02    262.07  3180.272971    385.74  3269.96   37.59   \n",
       "\n",
       "            T10YIE  DGS5  GOLDPMGBD230NLBM  SLVPRUSD  Fear&Greed  \n",
       "DATE                                                              \n",
       "2018-02-01    2.11  2.56           1078.58    17.190        30.0  \n",
       "2018-02-02    2.14  2.58           1071.18    17.135        15.0  \n",
       "2018-02-03    2.14  2.58           1071.18    17.135        40.0  \n",
       "2018-02-04    2.14  2.58           1071.18    17.135        24.0  \n",
       "2018-02-05    2.10  2.50           1075.01    16.875        11.0  \n",
       "...            ...   ...               ...       ...         ...  \n",
       "2020-10-27    1.71  0.34           1611.65    24.435        61.0  \n",
       "2020-10-28    1.70  0.34           1593.38    23.925        70.0  \n",
       "2020-10-29    1.71  0.38           1601.16    23.015        67.0  \n",
       "2020-10-30    1.70  0.38           1611.24    23.625        74.0  \n",
       "2020-10-31    1.70  0.38           1611.24    23.625        73.0  \n",
       "\n",
       "[1004 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = True\n",
    "\n",
    "Indicators_pct = Indicators.pct_change()\n",
    "date_ori = pd.to_datetime(Indicators_pct.index).tolist()\n",
    "\n",
    "if ROC:\n",
    "    Indicators_sca = Indicators.copy()\n",
    "    Indicators_full = pd.concat([Indicators_sca, Indicators_pct], axis=1)\n",
    "\n",
    "    minmax = MinMaxScaler().fit(Indicators_full.iloc[:, 0].values.reshape((-1,1)))\n",
    "    scaler = MinMaxScaler().fit(Indicators_full)\n",
    "    Indicators_full[:] = scaler.transform(Indicators_full)\n",
    "else:\n",
    "    minmax = MinMaxScaler().fit(Indicators.iloc[:, 0].values.reshape((-1,1)))\n",
    "    scaler = MinMaxScaler().fit(Indicators)\n",
    "    Indicators_full = Indicators.copy()\n",
    "    Indicators_full[:] = scaler.transform(Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmanr Result: correlation: 0.02246, p-value: 0.47746540643044133\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "c, p = spearmanr(list(Indicators_pct[\"CBBTCUSD\"])[1:], Indicators[\"Fear&Greed\"][:-1])\n",
    "print(\"Spearmanr Result: correlation: {:.5f}, p-value: {}\".format(c, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBBTCUSD</th>\n",
       "      <th>CBBCHUSD</th>\n",
       "      <th>CBCCIND</th>\n",
       "      <th>CBETHUSD</th>\n",
       "      <th>sp500</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>T10YIE</th>\n",
       "      <th>DGS5</th>\n",
       "      <th>GOLDPMGBD230NLBM</th>\n",
       "      <th>SLVPRUSD</th>\n",
       "      <th>...</th>\n",
       "      <th>CBBCHUSD</th>\n",
       "      <th>CBCCIND</th>\n",
       "      <th>CBETHUSD</th>\n",
       "      <th>sp500</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>T10YIE</th>\n",
       "      <th>DGS5</th>\n",
       "      <th>GOLDPMGBD230NLBM</th>\n",
       "      <th>SLVPRUSD</th>\n",
       "      <th>Fear&amp;Greed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-02</th>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.660487</td>\n",
       "      <td>0.767465</td>\n",
       "      <td>0.887114</td>\n",
       "      <td>0.390587</td>\n",
       "      <td>0.089922</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.824138</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.303910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409857</td>\n",
       "      <td>0.650246</td>\n",
       "      <td>0.508232</td>\n",
       "      <td>0.461613</td>\n",
       "      <td>0.373324</td>\n",
       "      <td>0.347308</td>\n",
       "      <td>0.459720</td>\n",
       "      <td>0.413060</td>\n",
       "      <td>0.612125</td>\n",
       "      <td>0.040210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-03</th>\n",
       "      <td>0.573470</td>\n",
       "      <td>0.713522</td>\n",
       "      <td>0.813643</td>\n",
       "      <td>0.948549</td>\n",
       "      <td>0.390587</td>\n",
       "      <td>0.089922</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.824138</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.303910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547670</td>\n",
       "      <td>0.748540</td>\n",
       "      <td>0.766999</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.623338</td>\n",
       "      <td>0.381119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-04</th>\n",
       "      <td>0.471966</td>\n",
       "      <td>0.645636</td>\n",
       "      <td>0.701513</td>\n",
       "      <td>0.795095</td>\n",
       "      <td>0.390587</td>\n",
       "      <td>0.089922</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.824138</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.303910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378410</td>\n",
       "      <td>0.493959</td>\n",
       "      <td>0.439515</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.623338</td>\n",
       "      <td>0.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05</th>\n",
       "      <td>0.352413</td>\n",
       "      <td>0.480534</td>\n",
       "      <td>0.552373</td>\n",
       "      <td>0.653347</td>\n",
       "      <td>0.306333</td>\n",
       "      <td>0.368458</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.083653</td>\n",
       "      <td>0.288507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224697</td>\n",
       "      <td>0.407552</td>\n",
       "      <td>0.420202</td>\n",
       "      <td>0.369083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303091</td>\n",
       "      <td>0.401742</td>\n",
       "      <td>0.511087</td>\n",
       "      <td>0.570162</td>\n",
       "      <td>0.033654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-06</th>\n",
       "      <td>0.426572</td>\n",
       "      <td>0.533151</td>\n",
       "      <td>0.624580</td>\n",
       "      <td>0.749968</td>\n",
       "      <td>0.340722</td>\n",
       "      <td>0.266286</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.803448</td>\n",
       "      <td>0.089987</td>\n",
       "      <td>0.284360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573281</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.871416</td>\n",
       "      <td>0.642498</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.517870</td>\n",
       "      <td>0.608801</td>\n",
       "      <td>0.075969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-27</th>\n",
       "      <td>0.995565</td>\n",
       "      <td>0.112575</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.344095</td>\n",
       "      <td>0.858453</td>\n",
       "      <td>0.313196</td>\n",
       "      <td>0.720238</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.819341</td>\n",
       "      <td>0.736374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488688</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.712444</td>\n",
       "      <td>0.546712</td>\n",
       "      <td>0.187919</td>\n",
       "      <td>0.320394</td>\n",
       "      <td>0.405380</td>\n",
       "      <td>0.501107</td>\n",
       "      <td>0.645710</td>\n",
       "      <td>0.089510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-28</th>\n",
       "      <td>0.955972</td>\n",
       "      <td>0.114787</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>0.769391</td>\n",
       "      <td>0.409660</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.794294</td>\n",
       "      <td>0.706161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484791</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.608895</td>\n",
       "      <td>0.395719</td>\n",
       "      <td>0.317714</td>\n",
       "      <td>0.320348</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.371024</td>\n",
       "      <td>0.550194</td>\n",
       "      <td>0.142096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-29</th>\n",
       "      <td>0.973015</td>\n",
       "      <td>0.114309</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.325989</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>0.372216</td>\n",
       "      <td>0.720238</td>\n",
       "      <td>0.065517</td>\n",
       "      <td>0.804960</td>\n",
       "      <td>0.652251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467335</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.664447</td>\n",
       "      <td>0.616787</td>\n",
       "      <td>0.120135</td>\n",
       "      <td>0.336109</td>\n",
       "      <td>0.623759</td>\n",
       "      <td>0.523365</td>\n",
       "      <td>0.490044</td>\n",
       "      <td>0.112138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-30</th>\n",
       "      <td>0.984657</td>\n",
       "      <td>0.111158</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.320670</td>\n",
       "      <td>0.768594</td>\n",
       "      <td>0.372216</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.065517</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.688389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450132</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.649329</td>\n",
       "      <td>0.504104</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.320348</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.536635</td>\n",
       "      <td>0.716222</td>\n",
       "      <td>0.135320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-31</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111104</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.323966</td>\n",
       "      <td>0.768594</td>\n",
       "      <td>0.372216</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.065517</td>\n",
       "      <td>0.818779</td>\n",
       "      <td>0.688389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470050</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.681740</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.623338</td>\n",
       "      <td>0.116755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1003 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CBBTCUSD  CBBCHUSD   CBCCIND  CBETHUSD     sp500    VIXCLS  \\\n",
       "DATE                                                                     \n",
       "2018-02-02  0.530630  0.660487  0.767465  0.887114  0.390587  0.089922   \n",
       "2018-02-03  0.573470  0.713522  0.813643  0.948549  0.390587  0.089922   \n",
       "2018-02-04  0.471966  0.645636  0.701513  0.795095  0.390587  0.089922   \n",
       "2018-02-05  0.352413  0.480534  0.552373  0.653347  0.306333  0.368458   \n",
       "2018-02-06  0.426572  0.533151  0.624580  0.749968  0.340722  0.266286   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2020-10-27  0.995565  0.112575  0.473957  0.344095  0.858453  0.313196   \n",
       "2020-10-28  0.955972  0.114787  0.473957  0.327273  0.769391  0.409660   \n",
       "2020-10-29  0.973015  0.114309  0.473957  0.325989  0.798480  0.372216   \n",
       "2020-10-30  0.984657  0.111158  0.473957  0.320670  0.768594  0.372216   \n",
       "2020-10-31  1.000000  0.111104  0.473957  0.323966  0.768594  0.372216   \n",
       "\n",
       "              T10YIE      DGS5  GOLDPMGBD230NLBM  SLVPRUSD  ...  CBBCHUSD  \\\n",
       "DATE                                                        ...             \n",
       "2018-02-02  0.976190  0.824138          0.078403  0.303910  ...  0.409857   \n",
       "2018-02-03  0.976190  0.824138          0.078403  0.303910  ...  0.547670   \n",
       "2018-02-04  0.976190  0.824138          0.078403  0.303910  ...  0.378410   \n",
       "2018-02-05  0.952381  0.796552          0.083653  0.288507  ...  0.224697   \n",
       "2018-02-06  0.952381  0.803448          0.089987  0.284360  ...  0.573281   \n",
       "...              ...       ...               ...       ...  ...       ...   \n",
       "2020-10-27  0.720238  0.051724          0.819341  0.736374  ...  0.488688   \n",
       "2020-10-28  0.714286  0.051724          0.794294  0.706161  ...  0.484791   \n",
       "2020-10-29  0.720238  0.065517          0.804960  0.652251  ...  0.467335   \n",
       "2020-10-30  0.714286  0.065517          0.818779  0.688389  ...  0.450132   \n",
       "2020-10-31  0.714286  0.065517          0.818779  0.688389  ...  0.470050   \n",
       "\n",
       "             CBCCIND  CBETHUSD     sp500    VIXCLS    T10YIE      DGS5  \\\n",
       "DATE                                                                     \n",
       "2018-02-02  0.650246  0.508232  0.461613  0.373324  0.347308  0.459720   \n",
       "2018-02-03  0.748540  0.766999  0.560872  0.168189  0.328205  0.448052   \n",
       "2018-02-04  0.493959  0.439515  0.560872  0.168189  0.328205  0.448052   \n",
       "2018-02-05  0.407552  0.420202  0.369083  1.000000  0.303091  0.401742   \n",
       "2018-02-06  0.823500  0.871416  0.642498  0.026666  0.328205  0.460000   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2020-10-27  0.671942  0.712444  0.546712  0.187919  0.320394  0.405380   \n",
       "2020-10-28  0.671942  0.608895  0.395719  0.317714  0.320348  0.448052   \n",
       "2020-10-29  0.671942  0.664447  0.616787  0.120135  0.336109  0.623759   \n",
       "2020-10-30  0.671942  0.649329  0.504104  0.168189  0.320348  0.448052   \n",
       "2020-10-31  0.671942  0.681740  0.560872  0.168189  0.328205  0.448052   \n",
       "\n",
       "            GOLDPMGBD230NLBM  SLVPRUSD  Fear&Greed  \n",
       "DATE                                                \n",
       "2018-02-02          0.413060  0.612125    0.040210  \n",
       "2018-02-03          0.477503  0.623338    0.381119  \n",
       "2018-02-04          0.477503  0.623338    0.055944  \n",
       "2018-02-05          0.511087  0.570162    0.033654  \n",
       "2018-02-06          0.517870  0.608801    0.075969  \n",
       "...                      ...       ...         ...  \n",
       "2020-10-27          0.501107  0.645710    0.089510  \n",
       "2020-10-28          0.371024  0.550194    0.142096  \n",
       "2020-10-29          0.523365  0.490044    0.112138  \n",
       "2020-10-30          0.536635  0.716222    0.135320  \n",
       "2020-10-31          0.477503  0.623338    0.116755  \n",
       "\n",
       "[1003 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log = Indicators_full[1:]\n",
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tangji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "epoch: 100 loss: 0.24402095 time: 0.003996610641479492\n",
      "epoch: 200 loss: 0.15860847 time: 0.0069959163665771484\n",
      "epoch: 300 loss: 0.10968199 time: 0.003997802734375\n",
      "epoch: 400 loss: 0.10722135 time: 0.005996227264404297\n",
      "epoch: 500 loss: 0.10086891 time: 0.003997802734375\n",
      "epoch: 600 loss: 0.07148639 time: 0.003997087478637695\n",
      "epoch: 700 loss: 0.065809995 time: 0.003997325897216797\n",
      "epoch: 800 loss: 0.06484715 time: 0.0039975643157958984\n",
      "epoch: 900 loss: 0.06476261 time: 0.0039975643157958984\n",
      "epoch: 1000 loss: 0.064567596 time: 0.0039975643157958984\n"
     ]
    }
   ],
   "source": [
    "thought_vector = autoencoder.reducedimension(df_log.values, 6, 0.001, 256, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 1000\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, learning_rate, num_layers, size, size_layer, output_size, forget_bias = 0.1):\n",
    "        \n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(size_layer) for _ in range(num_layers)], state_is_tuple = False)\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.compat.v1.nn.rnn_cell.DropoutWrapper(rnn_cells, output_keep_prob = forget_bias)\n",
    "        self.hidden_layer = tf.placeholder(tf.float32, (None, num_layers * 2 * size_layer))\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32)\n",
    "        rnn_W = tf.Variable(tf.random_normal((size_layer, output_size)))\n",
    "        rnn_B = tf.Variable(tf.random_normal([output_size]))\n",
    "        self.logits = tf.matmul(self.outputs[-1], rnn_W) + rnn_B\n",
    "        self.cost = tf.reduce_mean(tf.square(self.Y - self.logits))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl.LSTMCell object at 0x00000203D496FA48>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-12-1a1cff2689ee>:11: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\tangji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:966: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\tangji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:970: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tangji\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 avg loss: 0.00849157874676166\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "modelnn = model.Model(0.01, num_layers, thought_vector.shape[1], size_layer, 1, dropout_rate)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(epoch):\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    total_loss = 0\n",
    "    for k in range(0, (thought_vector.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        batch_x = np.expand_dims(thought_vector[k: k + timestamp, :], axis = 0)\n",
    "        batch_y = df_log.values[k + 1: k + timestamp + 1, 0].reshape([-1, 1])\n",
    "        last_state, _, loss = sess.run([modelnn.last_state, \n",
    "                                        modelnn.optimizer, \n",
    "                                        modelnn.cost], feed_dict={modelnn.X: batch_x, \n",
    "                                                                  modelnn.Y: batch_y, \n",
    "                                                                  modelnn.hidden_layer: init_value})\n",
    "        init_value = last_state\n",
    "        total_loss += loss\n",
    "    total_loss /= (thought_vector.shape[0] // timestamp)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print('epoch:', i + 1, 'avg loss:', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = (thought_vector.shape[0] // timestamp) * timestamp\n",
    "predict_days = len(df_log) - boundary\n",
    "\n",
    "output_predict = np.zeros(((thought_vector.shape[0] // timestamp) * timestamp, 1))\n",
    "init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "for k in range(0, (thought_vector.shape[0] // timestamp) * timestamp, timestamp):\n",
    "    out_logits, last_state = sess.run([modelnn.logits, modelnn.last_state], feed_dict = {modelnn.X:np.expand_dims(thought_vector[k: k + timestamp, :], axis = 0),\n",
    "                                     modelnn.hidden_layer: init_value})\n",
    "    init_value = last_state\n",
    "    output_predict[k: k + timestamp, :] = out_logits\n",
    "    \n",
    "out_logits, last_state = sess.run([modelnn.logits, modelnn.last_state], feed_dict = {modelnn.X:np.expand_dims(thought_vector[-predict_days:, :], axis = 0),\n",
    "                                  modelnn.hidden_layer: init_value})\n",
    "init_value = last_state\n",
    "output_predict_last = out_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.010203540522361193\n"
     ]
    }
   ],
   "source": [
    "print('Mean Square Error:', np.mean(np.square(output_predict[:, 0] - df_log.iloc[1: (thought_vector.shape[0] // timestamp) * timestamp + 1, 0].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jojo/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3285.0018330078683"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "from scipy import stats\n",
    "    \n",
    "Qs = range(0, 1)\n",
    "qs = range(0, 2)\n",
    "Ps = range(0, 2)\n",
    "ps = range(0, 2)\n",
    "D=1\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "best_aic = float(\"inf\")\n",
    "for param in parameters_list:\n",
    "#     try:\n",
    "    arima=sm.tsa.statespace.SARIMAX(df_log.iloc[:,0].values, order=(param[0], D, param[1]), seasonal_order=(param[2], D, param[3], 12)).fit(disp=-1)\n",
    "#     except:\n",
    "#         continue\n",
    "    aic = arima.aic\n",
    "    if aic < best_aic and aic:\n",
    "        best_arima = arima\n",
    "        best_aic = aic\n",
    "        \n",
    "best_aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_close(array):\n",
    "    return minmax.inverse_transform(array.reshape((-1,1))).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_arima' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc074fe73e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_arima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_arima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred_arima_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_arima\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpredict_days\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_arima' is not defined"
     ]
    }
   ],
   "source": [
    "last =10\n",
    "\n",
    "pred_arima = best_arima.predict()\n",
    "pred_arima_last = pred_arima[-predict_days:]\n",
    "x_range = np.arange(df_log.shape[0])[-last:]\n",
    "fig = plt.figure(figsize = (15,6))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(x_range, reverse_close(df_log.iloc[:,0].values)[-last:], label = 'true Close')\n",
    "ax.plot(x_range, reverse_close(pred_arima)[-last:], label = 'predict Close using Arima')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "ax.legend(loc = 'upper center', bbox_to_anchor= (0.5, -0.05), fancybox = True, shadow = True, ncol = 5)\n",
    "plt.xticks(x_range[::5], date_ori[::5][-last:])\n",
    "plt.title('overlap market Close')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_predict = np.vstack([pred_arima[:boundary], output_predict.reshape((-1))]).T\n",
    "stack_predict_last = np.vstack([pred_arima_last, output_predict_last.reshape((-1))]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_below_0 = np.where(stack_predict < 0)\n",
    "where_higher_1 = np.where(stack_predict > 1)\n",
    "stack_predict[where_below_0[0], where_below_0[1]] = 0\n",
    "stack_predict[where_higher_1[0], where_higher_1[1]] = 1\n",
    "\n",
    "where_below_0_last = np.where(stack_predict_last < 0)\n",
    "where_higher_1_last = np.where(stack_predict_last > 1)\n",
    "stack_predict_last[where_below_0_last[0], where_below_0_last[1]] = 0\n",
    "stack_predict_last[where_higher_1_last[0], where_higher_1_last[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(np.hstack([stack_predict, df_log.values[:boundary, 0].reshape((-1,1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(corr_df.corr(), annot= True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgd = {\n",
    "    'max_depth': 7,\n",
    "    'objective': 'reg:logistic',\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 10000\n",
    "    }\n",
    "train_Y = df_log.values[:boundary, 0]\n",
    "clf = xgb.XGBRegressor(**params_xgd)\n",
    "clf.fit(stack_predict,train_Y, eval_set=[(stack_predict,train_Y)], \n",
    "        eval_metric='rmse', early_stopping_rounds=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = clf.predict(stack_predict)\n",
    "stacked_last = clf.predict(stack_predict_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last =20\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "x_range = np.arange(boundary)[-last:]\n",
    "plt.plot(x_range, reverse_close(train_Y)[-last:], label = 'Real Close')\n",
    "plt.plot(x_range, reverse_close(pred_arima[:boundary])[-last:], label = 'ARIMA Close')\n",
    "plt.plot(x_range, reverse_close(output_predict)[-last:], label = 'RNN Close')\n",
    "plt.plot(x_range, reverse_close(stacked)[-last:], label = 'Stacked Close')\n",
    "plt.legend()\n",
    "plt.xticks(x_range[::5], date_ori[:boundary][-last:][::5])\n",
    "plt.title('stacked RNN + ARIMA with XG B')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "x_range = np.arange(5)\n",
    "plt.xticks(x_range, date_ori[-predict_days:])\n",
    "plt.plot(x_range, reverse_close(df_log.iloc[-predict_days:,0].values), label = 'Real Close')\n",
    "plt.plot(x_range, reverse_close(pred_arima_last), label = 'ARIMA Close')\n",
    "plt.plot(x_range, reverse_close(output_predict_last), label = 'RNN Close')\n",
    "plt.plot(x_range, reverse_close(stacked_last), label = 'Stacked Close')\n",
    "plt.legend()\n",
    "plt.title('Final prediction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

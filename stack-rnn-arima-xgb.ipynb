{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import autoencoder\n",
    "import model\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.alternative.me/fng/?limit=1500')\n",
    "data = json.loads(r.text)\n",
    "\n",
    "df_fearGreed = pd.DataFrame(data[\"data\"], columns=[\"timestamp\", \"value\"]).rename(columns={\"timestamp\": \"DATE\", \"value\": \"Fear&Greed\"})\n",
    "df_fearGreed[\"DATE\"] = pd.to_datetime(df_fearGreed[\"DATE\"], unit=\"s\")\n",
    "df_fearGreed[\"Fear&Greed\"] = df_fearGreed[\"Fear&Greed\"].astype(float)\n",
    "df_fearGreed = df_fearGreed.set_index(\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2018, 2, 1)\n",
    "end = datetime(2020, 10, 11)\n",
    "Indicators = DataReader([\"CBBTCUSD\", \"CBBCHUSD\", \"CBCCIND\", \"CBETHUSD\", \"sp500\", \"VIXCLS\", \"T10YIE\", \"DGS5\", \"GOLDPMGBD230NLBM\", \"SLVPRUSD\"], 'fred', start, end)\\\n",
    "    .fillna(method=\"pad\").fillna(method=\"backfill\").join(df_fearGreed).fillna(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicators.to_csv('dataset/Indicators.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicators = pd.read_csv('dataset/Indicators.csv', index_col=\"DATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBBTCUSD</th>\n",
       "      <th>CBBCHUSD</th>\n",
       "      <th>CBCCIND</th>\n",
       "      <th>CBETHUSD</th>\n",
       "      <th>sp500</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>T10YIE</th>\n",
       "      <th>DGS5</th>\n",
       "      <th>GOLDPMGBD230NLBM</th>\n",
       "      <th>SLVPRUSD</th>\n",
       "      <th>Fear&amp;Greed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>9014.23</td>\n",
       "      <td>1254.86</td>\n",
       "      <td>4512.850019</td>\n",
       "      <td>1017.48</td>\n",
       "      <td>2821.98</td>\n",
       "      <td>13.47</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1078.58</td>\n",
       "      <td>17.190</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>8787.52</td>\n",
       "      <td>1181.00</td>\n",
       "      <td>4455.297470</td>\n",
       "      <td>911.99</td>\n",
       "      <td>2762.13</td>\n",
       "      <td>17.31</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1071.18</td>\n",
       "      <td>17.135</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>9240.00</td>\n",
       "      <td>1269.71</td>\n",
       "      <td>4655.899898</td>\n",
       "      <td>969.40</td>\n",
       "      <td>2762.13</td>\n",
       "      <td>17.31</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1071.18</td>\n",
       "      <td>17.135</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>8167.91</td>\n",
       "      <td>1156.16</td>\n",
       "      <td>4168.798254</td>\n",
       "      <td>826.00</td>\n",
       "      <td>2762.13</td>\n",
       "      <td>17.31</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1071.18</td>\n",
       "      <td>17.135</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>6905.19</td>\n",
       "      <td>880.00</td>\n",
       "      <td>3520.919532</td>\n",
       "      <td>693.54</td>\n",
       "      <td>2648.94</td>\n",
       "      <td>37.32</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1075.01</td>\n",
       "      <td>16.875</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-10-07</td>\n",
       "      <td>10671.23</td>\n",
       "      <td>223.36</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>342.15</td>\n",
       "      <td>3419.45</td>\n",
       "      <td>28.06</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1601.83</td>\n",
       "      <td>23.530</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>10926.71</td>\n",
       "      <td>233.99</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>350.49</td>\n",
       "      <td>3446.83</td>\n",
       "      <td>26.36</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1607.73</td>\n",
       "      <td>23.960</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-10-09</td>\n",
       "      <td>11059.30</td>\n",
       "      <td>237.14</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>365.23</td>\n",
       "      <td>3477.13</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1628.99</td>\n",
       "      <td>24.315</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-10-10</td>\n",
       "      <td>11303.56</td>\n",
       "      <td>237.36</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>370.87</td>\n",
       "      <td>3477.13</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1628.99</td>\n",
       "      <td>24.315</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-10-11</td>\n",
       "      <td>11370.45</td>\n",
       "      <td>239.02</td>\n",
       "      <td>3180.272971</td>\n",
       "      <td>373.92</td>\n",
       "      <td>3477.13</td>\n",
       "      <td>25.00</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1628.99</td>\n",
       "      <td>24.315</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CBBTCUSD  CBBCHUSD      CBCCIND  CBETHUSD    sp500  VIXCLS  \\\n",
       "DATE                                                                     \n",
       "2018-02-01   9014.23   1254.86  4512.850019   1017.48  2821.98   13.47   \n",
       "2018-02-02   8787.52   1181.00  4455.297470    911.99  2762.13   17.31   \n",
       "2018-02-03   9240.00   1269.71  4655.899898    969.40  2762.13   17.31   \n",
       "2018-02-04   8167.91   1156.16  4168.798254    826.00  2762.13   17.31   \n",
       "2018-02-05   6905.19    880.00  3520.919532    693.54  2648.94   37.32   \n",
       "...              ...       ...          ...       ...      ...     ...   \n",
       "2020-10-07  10671.23    223.36  3180.272971    342.15  3419.45   28.06   \n",
       "2020-10-08  10926.71    233.99  3180.272971    350.49  3446.83   26.36   \n",
       "2020-10-09  11059.30    237.14  3180.272971    365.23  3477.13   25.00   \n",
       "2020-10-10  11303.56    237.36  3180.272971    370.87  3477.13   25.00   \n",
       "2020-10-11  11370.45    239.02  3180.272971    373.92  3477.13   25.00   \n",
       "\n",
       "            T10YIE  DGS5  GOLDPMGBD230NLBM  SLVPRUSD  Fear&Greed  \n",
       "DATE                                                              \n",
       "2018-02-01    2.11  2.56           1078.58    17.190        30.0  \n",
       "2018-02-02    2.14  2.58           1071.18    17.135        15.0  \n",
       "2018-02-03    2.14  2.58           1071.18    17.135        40.0  \n",
       "2018-02-04    2.14  2.58           1071.18    17.135        24.0  \n",
       "2018-02-05    2.10  2.50           1075.01    16.875        11.0  \n",
       "...            ...   ...               ...       ...         ...  \n",
       "2020-10-07    1.71  0.35           1601.83    23.530        43.0  \n",
       "2020-10-08    1.72  0.33           1607.73    23.960        46.0  \n",
       "2020-10-09    1.73  0.33           1628.99    24.315        48.0  \n",
       "2020-10-10    1.73  0.33           1628.99    24.315        53.0  \n",
       "2020-10-11    1.73  0.33           1628.99    24.315        55.0  \n",
       "\n",
       "[984 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = True\n",
    "\n",
    "Indicators_pct = Indicators.pct_change()\n",
    "date_ori = pd.to_datetime(Indicators_pct.index).tolist()\n",
    "\n",
    "if ROC:\n",
    "    Indicators_sca = Indicators.copy()\n",
    "    Indicators_full = pd.concat([Indicators_sca, Indicators_pct], axis=1)\n",
    "\n",
    "    minmax = MinMaxScaler().fit(Indicators_full.iloc[:, 0].values.reshape((-1,1)))\n",
    "    scaler = MinMaxScaler().fit(Indicators_full)\n",
    "    Indicators_full[:] = scaler.transform(Indicators_full)\n",
    "else:\n",
    "    minmax = MinMaxScaler().fit(Indicators.iloc[:, 0].values.reshape((-1,1)))\n",
    "    scaler = MinMaxScaler().fit(Indicators)\n",
    "    Indicators_full = Indicators.copy()\n",
    "    Indicators_full[:] = scaler.transform(Indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBBTCUSD</th>\n",
       "      <th>CBBCHUSD</th>\n",
       "      <th>CBCCIND</th>\n",
       "      <th>CBETHUSD</th>\n",
       "      <th>sp500</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>T10YIE</th>\n",
       "      <th>DGS5</th>\n",
       "      <th>GOLDPMGBD230NLBM</th>\n",
       "      <th>SLVPRUSD</th>\n",
       "      <th>...</th>\n",
       "      <th>CBBCHUSD</th>\n",
       "      <th>CBCCIND</th>\n",
       "      <th>CBETHUSD</th>\n",
       "      <th>sp500</th>\n",
       "      <th>VIXCLS</th>\n",
       "      <th>T10YIE</th>\n",
       "      <th>DGS5</th>\n",
       "      <th>GOLDPMGBD230NLBM</th>\n",
       "      <th>SLVPRUSD</th>\n",
       "      <th>Fear&amp;Greed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2018-02-02</td>\n",
       "      <td>0.576625</td>\n",
       "      <td>0.660487</td>\n",
       "      <td>0.767465</td>\n",
       "      <td>0.887114</td>\n",
       "      <td>0.390587</td>\n",
       "      <td>0.089922</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.824138</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.303910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409857</td>\n",
       "      <td>0.650246</td>\n",
       "      <td>0.508232</td>\n",
       "      <td>0.461613</td>\n",
       "      <td>0.373324</td>\n",
       "      <td>0.347308</td>\n",
       "      <td>0.459720</td>\n",
       "      <td>0.413060</td>\n",
       "      <td>0.612125</td>\n",
       "      <td>0.040210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-03</td>\n",
       "      <td>0.623179</td>\n",
       "      <td>0.713522</td>\n",
       "      <td>0.813643</td>\n",
       "      <td>0.948549</td>\n",
       "      <td>0.390587</td>\n",
       "      <td>0.089922</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.824138</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.303910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547670</td>\n",
       "      <td>0.748540</td>\n",
       "      <td>0.766999</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.623338</td>\n",
       "      <td>0.381119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-04</td>\n",
       "      <td>0.512876</td>\n",
       "      <td>0.645636</td>\n",
       "      <td>0.701513</td>\n",
       "      <td>0.795095</td>\n",
       "      <td>0.390587</td>\n",
       "      <td>0.089922</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.824138</td>\n",
       "      <td>0.078403</td>\n",
       "      <td>0.303910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378410</td>\n",
       "      <td>0.493959</td>\n",
       "      <td>0.439515</td>\n",
       "      <td>0.560872</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.623338</td>\n",
       "      <td>0.055944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>0.382960</td>\n",
       "      <td>0.480534</td>\n",
       "      <td>0.552373</td>\n",
       "      <td>0.653347</td>\n",
       "      <td>0.306333</td>\n",
       "      <td>0.368458</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.083653</td>\n",
       "      <td>0.288507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224697</td>\n",
       "      <td>0.407552</td>\n",
       "      <td>0.420202</td>\n",
       "      <td>0.369083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303091</td>\n",
       "      <td>0.401742</td>\n",
       "      <td>0.511087</td>\n",
       "      <td>0.570162</td>\n",
       "      <td>0.033654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>0.463548</td>\n",
       "      <td>0.533151</td>\n",
       "      <td>0.624580</td>\n",
       "      <td>0.749968</td>\n",
       "      <td>0.340722</td>\n",
       "      <td>0.266286</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.803448</td>\n",
       "      <td>0.089987</td>\n",
       "      <td>0.284360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573281</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.871416</td>\n",
       "      <td>0.642498</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.517870</td>\n",
       "      <td>0.608801</td>\n",
       "      <td>0.075969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>0.872773</td>\n",
       "      <td>0.118225</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.376455</td>\n",
       "      <td>0.940057</td>\n",
       "      <td>0.216592</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.868228</td>\n",
       "      <td>0.909064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451894</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.688843</td>\n",
       "      <td>0.550599</td>\n",
       "      <td>0.276313</td>\n",
       "      <td>0.350978</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.623338</td>\n",
       "      <td>0.118881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>0.898063</td>\n",
       "      <td>0.129441</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.420715</td>\n",
       "      <td>0.959663</td>\n",
       "      <td>0.212556</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>0.873903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540838</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.816968</td>\n",
       "      <td>0.596090</td>\n",
       "      <td>0.160288</td>\n",
       "      <td>0.298348</td>\n",
       "      <td>0.341373</td>\n",
       "      <td>0.501109</td>\n",
       "      <td>0.820024</td>\n",
       "      <td>0.118881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-02</td>\n",
       "      <td>0.844396</td>\n",
       "      <td>0.112378</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.381592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218820</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>0.865678</td>\n",
       "      <td>0.930095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370116</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>0.632787</td>\n",
       "      <td>0.180586</td>\n",
       "      <td>0.305303</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.443377</td>\n",
       "      <td>0.480174</td>\n",
       "      <td>0.135664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>0.724606</td>\n",
       "      <td>0.090048</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.321441</td>\n",
       "      <td>0.906375</td>\n",
       "      <td>0.316676</td>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.855108</td>\n",
       "      <td>0.882405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324981</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.470664</td>\n",
       "      <td>0.396478</td>\n",
       "      <td>0.358577</td>\n",
       "      <td>0.266074</td>\n",
       "      <td>0.333167</td>\n",
       "      <td>0.433491</td>\n",
       "      <td>0.521512</td>\n",
       "      <td>0.111298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-09-04</td>\n",
       "      <td>0.724606</td>\n",
       "      <td>0.090048</td>\n",
       "      <td>0.473957</td>\n",
       "      <td>0.321441</td>\n",
       "      <td>0.885458</td>\n",
       "      <td>0.316676</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.848774</td>\n",
       "      <td>0.877370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.470403</td>\n",
       "      <td>0.671942</td>\n",
       "      <td>0.669240</td>\n",
       "      <td>0.522808</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.368920</td>\n",
       "      <td>0.448052</td>\n",
       "      <td>0.451006</td>\n",
       "      <td>0.612264</td>\n",
       "      <td>0.041206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>946 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CBBTCUSD  CBBCHUSD   CBCCIND  CBETHUSD     sp500    VIXCLS  \\\n",
       "DATE                                                                     \n",
       "2018-02-02  0.576625  0.660487  0.767465  0.887114  0.390587  0.089922   \n",
       "2018-02-03  0.623179  0.713522  0.813643  0.948549  0.390587  0.089922   \n",
       "2018-02-04  0.512876  0.645636  0.701513  0.795095  0.390587  0.089922   \n",
       "2018-02-05  0.382960  0.480534  0.552373  0.653347  0.306333  0.368458   \n",
       "2018-02-06  0.463548  0.533151  0.624580  0.749968  0.340722  0.266286   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2020-08-31  0.872773  0.118225  0.473957  0.376455  0.940057  0.216592   \n",
       "2020-09-01  0.898063  0.129441  0.473957  0.420715  0.959663  0.212556   \n",
       "2020-09-02  0.844396  0.112378  0.473957  0.381592  1.000000  0.218820   \n",
       "2020-09-03  0.724606  0.090048  0.473957  0.321441  0.906375  0.316676   \n",
       "2020-09-04  0.724606  0.090048  0.473957  0.321441  0.885458  0.316676   \n",
       "\n",
       "              T10YIE      DGS5  GOLDPMGBD230NLBM  SLVPRUSD  ...  CBBCHUSD  \\\n",
       "DATE                                                        ...             \n",
       "2018-02-02  0.976190  0.824138          0.078403  0.303910  ...  0.409857   \n",
       "2018-02-03  0.976190  0.824138          0.078403  0.303910  ...  0.547670   \n",
       "2018-02-04  0.976190  0.824138          0.078403  0.303910  ...  0.378410   \n",
       "2018-02-05  0.952381  0.796552          0.083653  0.288507  ...  0.224697   \n",
       "2018-02-06  0.952381  0.803448          0.089987  0.284360  ...  0.573281   \n",
       "...              ...       ...               ...       ...  ...       ...   \n",
       "2020-08-31  0.773810  0.031034          0.868228  0.909064  ...  0.451894   \n",
       "2020-09-01  0.750000  0.024138          0.873903  1.000000  ...  0.540838   \n",
       "2020-09-02  0.732143  0.024138          0.865678  0.930095  ...  0.370116   \n",
       "2020-09-03  0.684524  0.017241          0.855108  0.882405  ...  0.324981   \n",
       "2020-09-04  0.714286  0.017241          0.848774  0.877370  ...  0.470403   \n",
       "\n",
       "             CBCCIND  CBETHUSD     sp500    VIXCLS    T10YIE      DGS5  \\\n",
       "DATE                                                                     \n",
       "2018-02-02  0.650246  0.508232  0.461613  0.373324  0.347308  0.459720   \n",
       "2018-02-03  0.748540  0.766999  0.560872  0.168189  0.328205  0.448052   \n",
       "2018-02-04  0.493959  0.439515  0.560872  0.168189  0.328205  0.448052   \n",
       "2018-02-05  0.407552  0.420202  0.369083  1.000000  0.303091  0.401742   \n",
       "2018-02-06  0.823500  0.871416  0.642498  0.026666  0.328205  0.460000   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2020-08-31  0.671942  0.688843  0.550599  0.276313  0.350978  0.448052   \n",
       "2020-09-01  0.671942  0.816968  0.596090  0.160288  0.298348  0.341373   \n",
       "2020-09-02  0.671942  0.549999  0.632787  0.180586  0.305303  0.448052   \n",
       "2020-09-03  0.671942  0.470664  0.396478  0.358577  0.266074  0.333167   \n",
       "2020-09-04  0.671942  0.669240  0.522808  0.168189  0.368920  0.448052   \n",
       "\n",
       "            GOLDPMGBD230NLBM  SLVPRUSD  Fear&Greed  \n",
       "DATE                                                \n",
       "2018-02-02          0.413060  0.612125    0.040210  \n",
       "2018-02-03          0.477503  0.623338    0.381119  \n",
       "2018-02-04          0.477503  0.623338    0.055944  \n",
       "2018-02-05          0.511087  0.570162    0.033654  \n",
       "2018-02-06          0.517870  0.608801    0.075969  \n",
       "...                      ...       ...         ...  \n",
       "2020-08-31          0.477503  0.623338    0.118881  \n",
       "2020-09-01          0.501109  0.820024    0.118881  \n",
       "2020-09-02          0.443377  0.480174    0.135664  \n",
       "2020-09-03          0.433491  0.521512    0.111298  \n",
       "2020-09-04          0.451006  0.612264    0.041206  \n",
       "\n",
       "[946 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log = Indicators_full[1:]\n",
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jojo/Documents/workspace/Python/BTC-prediction/autoencoder.py:8: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jojo/Documents/workspace/Python/BTC-prediction/autoencoder.py:11: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jojo/Documents/workspace/Python/BTC-prediction/autoencoder.py:29: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jojo/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/jojo/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/jojo/Documents/workspace/Python/BTC-prediction/autoencoder.py:30: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/jojo/Documents/workspace/Python/BTC-prediction/autoencoder.py:31: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "epoch: 10 loss: 0.29638305 time: 0.0017158985137939453\n",
      "epoch: 20 loss: 0.29620442 time: 0.0016932487487792969\n",
      "epoch: 30 loss: 0.2959003 time: 0.0017049312591552734\n",
      "epoch: 40 loss: 0.2953803 time: 0.002496957778930664\n",
      "epoch: 50 loss: 0.29448545 time: 0.0017237663269042969\n",
      "epoch: 60 loss: 0.29292804 time: 0.0017011165618896484\n",
      "epoch: 70 loss: 0.2901665 time: 0.0017459392547607422\n",
      "epoch: 80 loss: 0.28513402 time: 0.0017268657684326172\n",
      "epoch: 90 loss: 0.2758434 time: 0.0017011165618896484\n",
      "epoch: 100 loss: 0.26086426 time: 0.0016589164733886719\n",
      "epoch: 110 loss: 0.24539776 time: 0.0017430782318115234\n",
      "epoch: 120 loss: 0.23438907 time: 0.0020470619201660156\n",
      "epoch: 130 loss: 0.22006938 time: 0.0022318363189697266\n",
      "epoch: 140 loss: 0.19573052 time: 0.001706838607788086\n",
      "epoch: 150 loss: 0.18716784 time: 0.001894235610961914\n",
      "epoch: 160 loss: 0.1805417 time: 0.0017199516296386719\n",
      "epoch: 170 loss: 0.16286382 time: 0.0016567707061767578\n",
      "epoch: 180 loss: 0.14976057 time: 0.0016849040985107422\n",
      "epoch: 190 loss: 0.14656657 time: 0.0017027854919433594\n",
      "epoch: 200 loss: 0.14508952 time: 0.0016939640045166016\n",
      "epoch: 210 loss: 0.1428793 time: 0.0016379356384277344\n",
      "epoch: 220 loss: 0.1372015 time: 0.0016660690307617188\n",
      "epoch: 230 loss: 0.13489515 time: 0.001878976821899414\n",
      "epoch: 240 loss: 0.1300721 time: 0.001992940902709961\n",
      "epoch: 250 loss: 0.12266558 time: 0.0017402172088623047\n",
      "epoch: 260 loss: 0.11727249 time: 0.0016760826110839844\n",
      "epoch: 270 loss: 0.10317591 time: 0.002132892608642578\n",
      "epoch: 280 loss: 0.100528866 time: 0.0017237663269042969\n",
      "epoch: 290 loss: 0.09792581 time: 0.0017650127410888672\n",
      "epoch: 300 loss: 0.087243766 time: 0.0016908645629882812\n",
      "epoch: 310 loss: 0.07111797 time: 0.0017087459564208984\n",
      "epoch: 320 loss: 0.065946504 time: 0.0017452239990234375\n",
      "epoch: 330 loss: 0.06379566 time: 0.0017490386962890625\n",
      "epoch: 340 loss: 0.05439827 time: 0.00189208984375\n",
      "epoch: 350 loss: 0.037732303 time: 0.0019190311431884766\n",
      "epoch: 360 loss: 0.036526714 time: 0.0019741058349609375\n",
      "epoch: 370 loss: 0.03555526 time: 0.0018229484558105469\n",
      "epoch: 380 loss: 0.03478478 time: 0.002019166946411133\n",
      "epoch: 390 loss: 0.034182638 time: 0.0020401477813720703\n",
      "epoch: 400 loss: 0.033492878 time: 0.001817941665649414\n",
      "epoch: 410 loss: 0.032949816 time: 0.0018382072448730469\n",
      "epoch: 420 loss: 0.03228457 time: 0.0017151832580566406\n",
      "epoch: 430 loss: 0.03180029 time: 0.00174713134765625\n",
      "epoch: 440 loss: 0.03136756 time: 0.0017139911651611328\n",
      "epoch: 450 loss: 0.030945715 time: 0.0019419193267822266\n",
      "epoch: 460 loss: 0.03044765 time: 0.001795053482055664\n",
      "epoch: 470 loss: 0.030157672 time: 0.0017137527465820312\n",
      "epoch: 480 loss: 0.029924236 time: 0.0017940998077392578\n",
      "epoch: 490 loss: 0.029513767 time: 0.001920938491821289\n",
      "epoch: 500 loss: 0.029309463 time: 0.0017790794372558594\n",
      "epoch: 510 loss: 0.02882822 time: 0.002001047134399414\n",
      "epoch: 520 loss: 0.028608842 time: 0.0018680095672607422\n",
      "epoch: 530 loss: 0.028547693 time: 0.0017108917236328125\n",
      "epoch: 540 loss: 0.028186196 time: 0.0016968250274658203\n",
      "epoch: 550 loss: 0.02805353 time: 0.001714944839477539\n",
      "epoch: 560 loss: 0.027781349 time: 0.0021331310272216797\n",
      "epoch: 570 loss: 0.027606346 time: 0.001995086669921875\n",
      "epoch: 580 loss: 0.027486322 time: 0.0017757415771484375\n",
      "epoch: 590 loss: 0.02739029 time: 0.0017011165618896484\n",
      "epoch: 600 loss: 0.027145816 time: 0.0018968582153320312\n",
      "epoch: 610 loss: 0.027007928 time: 0.0018620491027832031\n",
      "epoch: 620 loss: 0.026964912 time: 0.0017096996307373047\n",
      "epoch: 630 loss: 0.026868375 time: 0.001688241958618164\n",
      "epoch: 640 loss: 0.026675265 time: 0.001683950424194336\n",
      "epoch: 650 loss: 0.026513169 time: 0.001728057861328125\n",
      "epoch: 660 loss: 0.026516128 time: 0.00168609619140625\n",
      "epoch: 670 loss: 0.026630241 time: 0.0018360614776611328\n",
      "epoch: 680 loss: 0.02641011 time: 0.0020759105682373047\n",
      "epoch: 690 loss: 0.026459614 time: 0.001779317855834961\n",
      "epoch: 700 loss: 0.026125366 time: 0.001802206039428711\n",
      "epoch: 710 loss: 0.026062012 time: 0.001873016357421875\n",
      "epoch: 720 loss: 0.026133573 time: 0.002290964126586914\n",
      "epoch: 730 loss: 0.025871456 time: 0.0020227432250976562\n",
      "epoch: 740 loss: 0.02597668 time: 0.002162933349609375\n",
      "epoch: 750 loss: 0.025746731 time: 0.0018689632415771484\n",
      "epoch: 760 loss: 0.025721392 time: 0.001676797866821289\n",
      "epoch: 770 loss: 0.025742857 time: 0.001744985580444336\n",
      "epoch: 780 loss: 0.025764145 time: 0.0018630027770996094\n",
      "epoch: 790 loss: 0.02558441 time: 0.0018372535705566406\n",
      "epoch: 800 loss: 0.025582775 time: 0.0022640228271484375\n",
      "epoch: 810 loss: 0.025566531 time: 0.0016748905181884766\n",
      "epoch: 820 loss: 0.025430765 time: 0.0019309520721435547\n",
      "epoch: 830 loss: 0.025366344 time: 0.0018169879913330078\n",
      "epoch: 840 loss: 0.025320334 time: 0.001886129379272461\n",
      "epoch: 850 loss: 0.025325818 time: 0.0019791126251220703\n",
      "epoch: 860 loss: 0.025319226 time: 0.0017192363739013672\n",
      "epoch: 870 loss: 0.025328148 time: 0.0016810894012451172\n",
      "epoch: 880 loss: 0.025124671 time: 0.0017290115356445312\n",
      "epoch: 890 loss: 0.02506866 time: 0.0018639564514160156\n",
      "epoch: 900 loss: 0.024900252 time: 0.0021789073944091797\n",
      "epoch: 910 loss: 0.024984332 time: 0.0020253658294677734\n",
      "epoch: 920 loss: 0.0252004 time: 0.0020589828491210938\n",
      "epoch: 930 loss: 0.02480279 time: 0.0018951892852783203\n",
      "epoch: 940 loss: 0.024932865 time: 0.0019600391387939453\n",
      "epoch: 950 loss: 0.02486748 time: 0.001901865005493164\n",
      "epoch: 960 loss: 0.024813445 time: 0.001970052719116211\n",
      "epoch: 970 loss: 0.024646092 time: 0.0017180442810058594\n",
      "epoch: 980 loss: 0.024807 time: 0.0017101764678955078\n",
      "epoch: 990 loss: 0.024586357 time: 0.0017001628875732422\n",
      "epoch: 1000 loss: 0.024601424 time: 0.0018351078033447266\n",
      "WARNING:tensorflow:From /Users/jojo/Documents/workspace/Python/BTC-prediction/autoencoder.py:40: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thought_vector = autoencoder.reducedimension(df_log.values, 6, 0.001, 128, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 1000\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fa6f9985090>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "epoch: 100 avg loss: 0.007650173226338861\n",
      "epoch: 200 avg loss: 0.005653369177678307\n",
      "epoch: 300 avg loss: 0.005611861725096836\n",
      "epoch: 400 avg loss: 0.0035226123574673652\n",
      "epoch: 500 avg loss: 0.0031911661557074617\n",
      "epoch: 600 avg loss: 0.003139555380555741\n",
      "epoch: 700 avg loss: 0.41207488095738315\n",
      "epoch: 800 avg loss: 0.027528704228360013\n",
      "epoch: 900 avg loss: 0.03313394525289906\n",
      "epoch: 1000 avg loss: 0.006573567265189022\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "modelnn = model.Model(0.01, num_layers, thought_vector.shape[1], size_layer, 1, dropout_rate)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(epoch):\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    total_loss = 0\n",
    "    for k in range(0, (thought_vector.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        batch_x = np.expand_dims(thought_vector[k: k + timestamp, :], axis = 0)\n",
    "        batch_y = df_log.values[k + 1: k + timestamp + 1, 0].reshape([-1, 1])\n",
    "        last_state, _, loss = sess.run([modelnn.last_state, \n",
    "                                        modelnn.optimizer, \n",
    "                                        modelnn.cost], feed_dict={modelnn.X: batch_x, \n",
    "                                                                  modelnn.Y: batch_y, \n",
    "                                                                  modelnn.hidden_layer: init_value})\n",
    "        init_value = last_state\n",
    "        total_loss += loss\n",
    "    total_loss /= (thought_vector.shape[0] // timestamp)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print('epoch:', i + 1, 'avg loss:', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = (thought_vector.shape[0] // timestamp) * timestamp\n",
    "predict_days = len(df_log) - boundary\n",
    "\n",
    "output_predict = np.zeros(((thought_vector.shape[0] // timestamp) * timestamp, 1))\n",
    "init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "for k in range(0, (thought_vector.shape[0] // timestamp) * timestamp, timestamp):\n",
    "    out_logits, last_state = sess.run([modelnn.logits, modelnn.last_state], feed_dict = {modelnn.X:np.expand_dims(thought_vector[k: k + timestamp, :], axis = 0),\n",
    "                                     modelnn.hidden_layer: init_value})\n",
    "    init_value = last_state\n",
    "    output_predict[k: k + timestamp, :] = out_logits\n",
    "    \n",
    "out_logits, last_state = sess.run([modelnn.logits, modelnn.last_state], feed_dict = {modelnn.X:np.expand_dims(thought_vector[-predict_days:, :], axis = 0),\n",
    "                                  modelnn.hidden_layer: init_value})\n",
    "init_value = last_state\n",
    "output_predict_last = out_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error: 0.010203540522361193\n"
     ]
    }
   ],
   "source": [
    "print('Mean Square Error:', np.mean(np.square(output_predict[:, 0] - df_log.iloc[1: (thought_vector.shape[0] // timestamp) * timestamp + 1, 0].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jojo/opt/anaconda3/lib/python3.7/site-packages/statsmodels/base/model.py:568: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3285.0018330078683"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from itertools import product\n",
    "from scipy import stats\n",
    "    \n",
    "Qs = range(0, 1)\n",
    "qs = range(0, 2)\n",
    "Ps = range(0, 2)\n",
    "ps = range(0, 2)\n",
    "D=1\n",
    "parameters = product(ps, qs, Ps, Qs)\n",
    "parameters_list = list(parameters)\n",
    "best_aic = float(\"inf\")\n",
    "for param in parameters_list:\n",
    "#     try:\n",
    "    arima=sm.tsa.statespace.SARIMAX(df_log.iloc[:,0].values, order=(param[0], D, param[1]), seasonal_order=(param[2], D, param[3], 12)).fit(disp=-1)\n",
    "#     except:\n",
    "#         continue\n",
    "    aic = arima.aic\n",
    "    if aic < best_aic and aic:\n",
    "        best_arima = arima\n",
    "        best_aic = aic\n",
    "        \n",
    "best_aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_close(array):\n",
    "    return minmax.inverse_transform(array.reshape((-1,1))).reshape((-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_arima' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc074fe73e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpred_arima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_arima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred_arima_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_arima\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpredict_days\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_arima' is not defined"
     ]
    }
   ],
   "source": [
    "last =10\n",
    "\n",
    "pred_arima = best_arima.predict()\n",
    "pred_arima_last = pred_arima[-predict_days:]\n",
    "x_range = np.arange(df_log.shape[0])[-last:]\n",
    "fig = plt.figure(figsize = (15,6))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(x_range, reverse_close(df_log.iloc[:,0].values)[-last:], label = 'true Close')\n",
    "ax.plot(x_range, reverse_close(pred_arima)[-last:], label = 'predict Close using Arima')\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0 + box.height * 0.1, box.width, box.height * 0.9])\n",
    "ax.legend(loc = 'upper center', bbox_to_anchor= (0.5, -0.05), fancybox = True, shadow = True, ncol = 5)\n",
    "plt.xticks(x_range[::5], date_ori[::5][-last:])\n",
    "plt.title('overlap market Close')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_predict = np.vstack([pred_arima[:boundary], output_predict.reshape((-1))]).T\n",
    "stack_predict_last = np.vstack([pred_arima_last, output_predict_last.reshape((-1))]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_below_0 = np.where(stack_predict < 0)\n",
    "where_higher_1 = np.where(stack_predict > 1)\n",
    "stack_predict[where_below_0[0], where_below_0[1]] = 0\n",
    "stack_predict[where_higher_1[0], where_higher_1[1]] = 1\n",
    "\n",
    "where_below_0_last = np.where(stack_predict_last < 0)\n",
    "where_higher_1_last = np.where(stack_predict_last > 1)\n",
    "stack_predict_last[where_below_0_last[0], where_below_0_last[1]] = 0\n",
    "stack_predict_last[where_higher_1_last[0], where_higher_1_last[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(np.hstack([stack_predict, df_log.values[:boundary, 0].reshape((-1,1))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(corr_df.corr(), annot= True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_xgd = {\n",
    "    'max_depth': 7,\n",
    "    'objective': 'reg:logistic',\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 10000\n",
    "    }\n",
    "train_Y = df_log.values[:boundary, 0]\n",
    "clf = xgb.XGBRegressor(**params_xgd)\n",
    "clf.fit(stack_predict,train_Y, eval_set=[(stack_predict,train_Y)], \n",
    "        eval_metric='rmse', early_stopping_rounds=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = clf.predict(stack_predict)\n",
    "stacked_last = clf.predict(stack_predict_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last =20\n",
    "\n",
    "plt.figure(figsize = (15,6))\n",
    "x_range = np.arange(boundary)[-last:]\n",
    "plt.plot(x_range, reverse_close(train_Y)[-last:], label = 'Real Close')\n",
    "plt.plot(x_range, reverse_close(pred_arima[:boundary])[-last:], label = 'ARIMA Close')\n",
    "plt.plot(x_range, reverse_close(output_predict)[-last:], label = 'RNN Close')\n",
    "plt.plot(x_range, reverse_close(stacked)[-last:], label = 'Stacked Close')\n",
    "plt.legend()\n",
    "plt.xticks(x_range[::5], date_ori[:boundary][-last:][::5])\n",
    "plt.title('stacked RNN + ARIMA with XG B')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "x_range = np.arange(5)\n",
    "plt.xticks(x_range, date_ori[-predict_days:])\n",
    "plt.plot(x_range, reverse_close(df_log.iloc[-predict_days:,0].values), label = 'Real Close')\n",
    "plt.plot(x_range, reverse_close(pred_arima_last), label = 'ARIMA Close')\n",
    "plt.plot(x_range, reverse_close(output_predict_last), label = 'RNN Close')\n",
    "plt.plot(x_range, reverse_close(stacked_last), label = 'Stacked Close')\n",
    "plt.legend()\n",
    "plt.title('Final prediction')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
